{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DC-GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Wb14fGw_q3p2",
        "CVlDCXk9qyvO",
        "nlR5cd_AqTWc",
        "MJSgbYharvxc",
        "rpc_GHOW66tz",
        "68M6JxMy14eZ",
        "heUxW-_xr82Y",
        "VdYgbcFFxAP8",
        "feIkHvP1avir",
        "MxwS0RIWb9ME"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOihmFafmsah/vHOOcxf9GB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romina-a/JAX-GAN/blob/AddingStaxAdam/DC_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGxYkbXypSFs"
      },
      "source": [
        "# GAN using STAX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb14fGw_q3p2"
      },
      "source": [
        "#Defining data loaders using Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MEpILnoq2rC"
      },
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~ UTILS FOR LOADING DATA WITH PYTORCH ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "from torch.utils import data\n",
        "from torchvision.datasets import MNIST, CIFAR10\n",
        "\n",
        "\n",
        "def numpy_collate(batch):\n",
        "    if isinstance(batch[0], np.ndarray):\n",
        "        return np.stack(batch)\n",
        "    elif isinstance(batch[0], (tuple, list)):\n",
        "        transposed = zip(*batch)\n",
        "        return [numpy_collate(samples) for samples in transposed]\n",
        "    else:\n",
        "        return np.array(batch)\n",
        "\n",
        "\n",
        "class NumpyLoader(data.DataLoader):\n",
        "    def __init__(self, dataset, batch_size=1,\n",
        "                 shuffle=False, sampler=None,\n",
        "                 batch_sampler=None, num_workers=0,\n",
        "                 pin_memory=False, drop_last=False,\n",
        "                 timeout=0, worker_init_fn=None):\n",
        "        super(self.__class__, self).__init__(dataset,\n",
        "                                             batch_size=batch_size,\n",
        "                                             shuffle=shuffle,\n",
        "                                             sampler=sampler,\n",
        "                                             batch_sampler=batch_sampler,\n",
        "                                             num_workers=num_workers,\n",
        "                                             collate_fn=numpy_collate,\n",
        "                                             pin_memory=pin_memory,\n",
        "                                             drop_last=drop_last,\n",
        "                                             timeout=timeout,\n",
        "                                             worker_init_fn=worker_init_fn)\n",
        "\n",
        "\n",
        "def flatten_and_cast(pic):\n",
        "    return np.ravel(np.array(pic, dtype=jnp.float32))\n",
        "\n",
        "\n",
        "def cast(pic):\n",
        "    pic = (np.array(pic, dtype=jnp.float32)-127.5)/127.5\n",
        "    if len(pic.shape) == 2:\n",
        "        pic = pic[..., np.newaxis]\n",
        "    return pic\n",
        "\n",
        "\n",
        "def mnist_dataset(batch_size, digit=None, flatten=False):\n",
        "    data_adr = \"\"\n",
        "    if flatten:\n",
        "        mnist_dataset = MNIST('./tmp/mnist/', download=True, transform=flatten_and_cast)\n",
        "    else:\n",
        "        mnist_dataset = MNIST('./tmp/mnist/', download=True, transform=cast)\n",
        "    if digit is not None:\n",
        "        idx = mnist_dataset.targets == digit\n",
        "        mnist_dataset.data = mnist_dataset.data[idx]\n",
        "        mnist_dataset.targets = mnist_dataset.targets[idx]\n",
        "    # load training with the generator (makes batch easier I think)\n",
        "    training_generator = NumpyLoader(mnist_dataset, batch_size=batch_size, num_workers=0)\n",
        "    return training_generator\n",
        "\n",
        "\n",
        "def cifar10_dataset(batch_size, digit=None):\n",
        "    data_adr = \"\"\n",
        "    cifar10_dataset = CIFAR10('./tmp/cifar10/', download=True, transform=cast)\n",
        "    if digit is not None:\n",
        "        idx = np.array(cifar10_dataset.targets) == digit\n",
        "        cifar10_dataset.data = cifar10_dataset.data[idx]\n",
        "        cifar10_dataset.targets = np.array(cifar10_dataset.targets)[idx]\n",
        "    # load training with the generator (makes batch easier I think)\n",
        "    training_generator = NumpyLoader(cifar10_dataset, batch_size=batch_size, num_workers=0)\n",
        "    return training_generator\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVlDCXk9qyvO"
      },
      "source": [
        "# Defining additional Stax layers and BCE loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BYiO5dvo6Lx"
      },
      "source": [
        "from jax.nn.initializers import normal\n",
        "from jax.nn import leaky_relu, sigmoid\n",
        "from jax.experimental import stax\n",
        "from jax.experimental.stax import (BatchNorm, Conv, ConvTranspose, Dense,\n",
        "                                   Tanh, Relu, Flatten, Sigmoid)\n",
        "from jax.experimental.optimizers import pack_optimizer_state, unpack_optimizer_state\n",
        "import jax.numpy as jnp\n",
        "import jax.random as random\n",
        "\n",
        "from jax.lax import sort\n",
        "\n",
        "from jax import value_and_grad, jit\n",
        "from functools import partial\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "finfo = jnp.finfo(jnp.float32)\n",
        "EPS = finfo.eps\n",
        "EPSNEG = finfo.epsneg\n",
        "\n",
        "# ~~~~~~~~~~~~ losses ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "def BCE_from_logits(logits, targets):\n",
        "    p = sigmoid(logits)\n",
        "    loss_array = -jnp.log(jnp.where(p == 0, EPS, p)) * targets\\\n",
        "                 - jnp.log(1 - jnp.where(p == 1, 1-EPSNEG, p)) * (1 - targets)\n",
        "    return jnp.mean(loss_array)\n",
        "\n",
        "# ------------- layers with stax convention --------------------------\n",
        "def Reshape(output_shape):\n",
        "    def init_fun(rng, input_shape):\n",
        "        size_in = 1\n",
        "        for a in input_shape[1:]: size_in = size_in * a\n",
        "        size_out = 1\n",
        "        for a in output_shape: size_out = size_out * a\n",
        "        assert size_out == size_in, \"input and output sizes must match\"\n",
        "        return (input_shape[0], *output_shape[:]), ()\n",
        "\n",
        "    def apply_fun(params, inputs, **kwargs):\n",
        "        return jnp.reshape(inputs, (inputs.shape[0], *output_shape))\n",
        "\n",
        "    return init_fun, apply_fun\n",
        "\n",
        "def LeakyRelu(negative_slope):\n",
        "    return stax.elementwise(leaky_relu, negative_slope=negative_slope)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5YVTkgNp8Z3"
      },
      "source": [
        "defining generator and discriminator models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_GCGbBHqFmf"
      },
      "source": [
        "# -----------------------------------   Network Models   ------------------------------------\n",
        "def conv_generator_mnist():\n",
        "    model = stax.serial(\n",
        "        Dense(1024 * 7 * 7),\n",
        "        Reshape((7, 7, 1024)),\n",
        "        ConvTranspose(out_chan=512, filter_shape=(5, 5), strides=(1, 1),\n",
        "                      padding='SAME', W_init=normal(2e-2), b_init=normal(2e-2)),\n",
        "        BatchNorm(), Relu,\n",
        "        ConvTranspose(out_chan=256, filter_shape=(5, 5), strides=(2, 2),\n",
        "                      padding='SAME', W_init=normal(2e-2), b_init=normal(2e-2)),\n",
        "        BatchNorm(), Relu,\n",
        "        ConvTranspose(out_chan=128, filter_shape=(5, 5), strides=(2, 2),\n",
        "                      padding='SAME', W_init=normal(2e-2), b_init=normal(2e-2)),\n",
        "        BatchNorm(), Relu,\n",
        "        ConvTranspose(out_chan=1, filter_shape=(5, 5), strides=(1, 1),\n",
        "                      padding='SAME', W_init=normal(2e-2), b_init=normal(2e-2)),\n",
        "        Tanh,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def conv_generator_cifar10():\n",
        "    model = stax.serial(\n",
        "        Dense(1024 * 2 * 2),\n",
        "        Reshape((2, 2, 1024)),\n",
        "        ConvTranspose(out_chan=512, filter_shape=(5, 5), strides=(2, 2),\n",
        "                      padding='SAME', W_init=normal(2e-2), b_init=normal(2e-2)),\n",
        "        BatchNorm(), Relu,\n",
        "        ConvTranspose(out_chan=256, filter_shape=(5, 5), strides=(2, 2),\n",
        "                      padding='SAME', W_init=normal(2e-2), b_init=normal(2e-2)),\n",
        "        BatchNorm(), Relu,\n",
        "        ConvTranspose(out_chan=128, filter_shape=(5, 5), strides=(2, 2),\n",
        "                      padding='SAME', W_init=normal(2e-2), b_init=normal(2e-2)),\n",
        "        BatchNorm(), Relu,\n",
        "        ConvTranspose(out_chan=3, filter_shape=(5, 5), strides=(2, 2),\n",
        "                      padding='SAME', W_init=normal(2e-2), b_init=normal(2e-2)),\n",
        "        Tanh,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def conv_discriminator():\n",
        "    model = stax.serial(\n",
        "        Conv(out_chan=64, filter_shape=(5, 5), strides=(2, 2),\n",
        "             padding='SAME', W_init=normal(2e-2), b_init=normal(1e-6)),\n",
        "        LeakyRelu(negative_slope=0.2),\n",
        "        Conv(out_chan=128, filter_shape=(5, 5), strides=(2, 2),\n",
        "             padding='SAME', W_init=normal(2e-2), b_init=normal(1e-6)),\n",
        "        BatchNorm(), LeakyRelu(negative_slope=0.2),\n",
        "        Conv(out_chan=256, filter_shape=(5, 5), strides=(2, 2),\n",
        "             padding='SAME', W_init=normal(2e-2), b_init=normal(1e-6)),\n",
        "        BatchNorm(), LeakyRelu(negative_slope=0.2),\n",
        "        Conv(out_chan=512, filter_shape=(5, 5), strides=(2, 2),\n",
        "             padding='SAME', W_init=normal(2e-2), b_init=normal(1e-6)),\n",
        "        BatchNorm(), LeakyRelu(negative_slope=0.2), Flatten,\n",
        "        Dense(1),\n",
        "        # Sigmoid\n",
        "    )\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlR5cd_AqTWc"
      },
      "source": [
        "# GAN\n",
        "\n",
        "Defining GAN Class. The static methods are to save and load GANs as pickled objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwd8kLVLptm2"
      },
      "source": [
        "# ----------------------------------- GAN --------------------------------------------\n",
        "class GAN:\n",
        "    r\"\"\"\n",
        "    GAN implementation using jax.experimental\n",
        "    generator and discriminator are jax.experimental.stax models: (init_func, apply_func) pairs\n",
        "    optimizers are jax.experimental.optimizers optimizers: (init, update, get_params) triplets\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def save_gan_to_file(gan, d_state, g_state, save_adr):\n",
        "        params = {'d_creator': gan.d_creator,\n",
        "                  'g_creator': gan.g_creator,\n",
        "                  'd_opt_creator': gan.d_opt_creator,\n",
        "                  'g_opt_creator': gan.g_opt_creator,\n",
        "                  'loss_function': gan.loss_function,\n",
        "                  'batch_size': gan.batch_size,\n",
        "                  'd_input_shape': gan.d_input_shape,\n",
        "                  'g_input_shape': gan.g_input_shape,\n",
        "                  'd_output_shape': gan.d_output_shape,\n",
        "                  'g_output_shape': gan.g_output_shape,\n",
        "                  'g_state': unpack_optimizer_state(g_state),\n",
        "                  'd_state': unpack_optimizer_state(d_state)\n",
        "                  }\n",
        "        with open(save_adr, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_gan_from_file(load_adr):\n",
        "        with open(load_adr, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        if params['d_state'] is not None:\n",
        "            params['d_state'] = pack_optimizer_state(params['d_state'])\n",
        "        if params['g_state'] is not None:\n",
        "            params['g_state'] = pack_optimizer_state(params['g_state'])\n",
        "        gan = GAN(params['d_creator'], params['g_creator'], params['d_opt_creator'], params['g_opt_creator'],\n",
        "                  params['loss_function'])\n",
        "        gan.d_output_shape = params['d_output_shape']\n",
        "        gan.g_output_shape = params['g_output_shape']\n",
        "        gan.d_input_shape = params['d_input_shape']\n",
        "        gan.g_input_shape = params['g_input_shape']\n",
        "        gan.batch_size = params['batch_size']\n",
        "        return gan, params['d_state'], params['g_state']\n",
        "\n",
        "    def __init__(self, d_creator, g_creator, d_opt_creator, g_opt_creator, loss_function):\n",
        "        \"\"\"\n",
        "\n",
        "        :param d_creator: (callable) with no input returns discriminator stax model: (init_func, apply_func)\n",
        "        :param g_creator: (callable) with no input returns generator stax model: (init_func, apply_func)\n",
        "        :param d_opt_creator: (callable) with no input returns discriminator optimizer: (init, update, get_params)\n",
        "        :param g_opt_creator: (callable) with no input returns generator optimizer: (init, update, get_params)\n",
        "        :param loss_function: (function) to calculate loss from discriminator outputs:\n",
        "                      (discriminator-outputs, real-labels)-> loss\n",
        "        \"\"\"\n",
        "        d_init, d_apply = d_creator()\n",
        "        g_init, g_apply = g_creator()\n",
        "        (d_opt_init, d_opt_update, d_opt_get_params) = d_opt_creator()\n",
        "        (g_opt_init, g_opt_update, g_opt_get_params) = g_opt_creator()\n",
        "\n",
        "        # self.creators = {'d_creator': d_creator,\n",
        "        #                  'g_creator': g_creator,\n",
        "        #                  'd_opt_creator': d_opt_creator,\n",
        "        #                  'g_opt_creator': g_opt_creator\n",
        "        #                  }\n",
        "        self.d_creator = d_creator\n",
        "        self.g_creator = g_creator\n",
        "        self.d_opt_creator = d_opt_creator\n",
        "        self.g_opt_creator = g_opt_creator\n",
        "        self.d = {'init': d_init, 'apply': d_apply}\n",
        "        self.g = {'init': g_init, 'apply': g_apply}\n",
        "        self.d_opt = {'init': d_opt_init, 'update': d_opt_update, 'get_params': d_opt_get_params}\n",
        "        self.g_opt = {'init': g_opt_init, 'update': g_opt_update, 'get_params': g_opt_get_params}\n",
        "        self.loss_function = loss_function\n",
        "        self.d_output_shape = None\n",
        "        self.g_output_shape = None\n",
        "        self.d_input_shape = None\n",
        "        self.g_input_shape = None\n",
        "        self.batch_size = None\n",
        "\n",
        "    def init(self, prng_d, prng_g, d_input_shape, g_input_shape, batch_size):\n",
        "        \"\"\"\n",
        "\n",
        "        :param prng_d: (jax.PRNGKey) for discriminator initialization\n",
        "        :param prng_g: (jax.PRNGKey) for generator initialization\n",
        "        :param d_input_shape: (tuple) shape of the discriminator input excluding batch size\n",
        "        :param g_input_shape: (tuple) shape of the generator input excluding batch size\n",
        "        :param batch_size: (int) used for initialization and training\n",
        "        :return: discriminator and generator states (needed for train_step and generate_samples)\n",
        "        \"\"\"\n",
        "        self.g_input_shape = g_input_shape\n",
        "        self.d_input_shape = d_input_shape\n",
        "        self.d_output_shape, d_params = self.d['init'](prng_d, (batch_size, *d_input_shape))\n",
        "        self.g_output_shape, g_params = self.g['init'](prng_g, (batch_size, *g_input_shape))\n",
        "        self.batch_size = batch_size\n",
        "        d_state = self.d_opt['init'](d_params)\n",
        "        g_state = self.g_opt['init'](g_params)\n",
        "        return d_state, g_state\n",
        "\n",
        "    @partial(jit, static_argnums=(0,))\n",
        "    def _d_loss(self, d_params, g_params, z, real_samples):\n",
        "        fake_ims = self.g['apply'](g_params, z)\n",
        "\n",
        "        fake_predictions = self.d['apply'](d_params, fake_ims)\n",
        "        real_predictions = self.d['apply'](d_params, real_samples)\n",
        "        fake_loss = self.loss_function(fake_predictions, jnp.zeros_like(fake_predictions))\n",
        "        real_loss = self.loss_function(real_predictions, jnp.ones_like(real_predictions))\n",
        "\n",
        "        return fake_loss + real_loss\n",
        "\n",
        "    @partial(jit, static_argnums=(0, 4))\n",
        "    def _g_loss(self, g_params, d_params, z, k):\n",
        "        fake_ims = self.g['apply'](g_params, z)\n",
        "\n",
        "        fake_predictions = self.d['apply'](d_params, fake_ims)\n",
        "        fake_predictions = sort(fake_predictions, 0)\n",
        "        fake_predictions = jnp.flip(fake_predictions, 0)\n",
        "        fake_predictions = fake_predictions[:k]\n",
        "\n",
        "        loss = self.loss_function(fake_predictions, jnp.ones_like(fake_predictions))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    @partial(jit, static_argnums=(0, 6))\n",
        "    def train_step(self, i, prng_key, d_state, g_state, real_samples, k):\n",
        "        \"\"\"\n",
        "        !: call init function before train_step\n",
        "\n",
        "        :param i: (int) step number\n",
        "        :param prng_key: (jax.random.PRNGKey) used to create random samples from the generator\n",
        "        :param d_state: previous discriminator state\n",
        "        :param g_state: previous generator state\n",
        "        :param real_samples: (np/jnp array) samples form the training set\n",
        "        :param k: (int) to choose top k for training generator, if None all elements are chosen\n",
        "        :return: updated discriminator and generator states and discriminator and generator loss values\n",
        "        \"\"\"\n",
        "        k = k or self.batch_size\n",
        "        prng1, prng2 = random.split(prng_key, 2)\n",
        "        d_params = self.d_opt['get_params'](d_state)\n",
        "        g_params = self.g_opt['get_params'](g_state)\n",
        "\n",
        "        z = random.normal(prng1, (self.batch_size, *self.g_input_shape))\n",
        "        d_loss_value, d_grads = value_and_grad(self._d_loss)(d_params, g_params, z, real_samples)\n",
        "        d_state = self.d_opt['update'](i, d_grads, d_state)\n",
        "\n",
        "        z = random.normal(prng2, (self.batch_size, *self.g_input_shape))\n",
        "        g_loss_value, g_grads = value_and_grad(self._g_loss)(g_params, d_params, z, k)\n",
        "        g_state = self.g_opt['update'](i, g_grads, g_state)\n",
        "\n",
        "        return d_state, g_state, d_loss_value, g_loss_value\n",
        "\n",
        "    @partial(jit, static_argnums=(0,))\n",
        "    def generate_samples(self, z, g_state):\n",
        "        \"\"\"\n",
        "\n",
        "        :param z: (np/jnp array) shape: (n, generator_input_dims)\n",
        "        :param g_state: generator state\n",
        "        :return: (jnp array) shape: (n, generator_output_dims) n generated samples\n",
        "        \"\"\"\n",
        "        fakes = self.g['apply'](self.g_opt['get_params'](g_state), z)\n",
        "        return fakes\n",
        "\n",
        "    @partial(jit, static_argnums=(0,))\n",
        "    def rate_samples(self, samples, d_state):\n",
        "        \"\"\"\n",
        "\n",
        "        :return: (jnp array) shape: (n, 1) discriminator ratings for the samples\n",
        "        \"\"\"\n",
        "        rates = self.d['apply'](self.d_opt['get_params'](d_state), samples)\n",
        "        return rates\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDUlhB6Nr3Nq"
      },
      "source": [
        "setting Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJSgbYharvxc"
      },
      "source": [
        "# Defining the training hyper parameters\n",
        "\n",
        "You can change the dataset to mnist here.\n",
        "I set the training iterations to 40000 as the project report. I recommed reducing for demonstration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6j7XcUprin2"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.experimental.optimizers import adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import time\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "dataset = 'cifar10' \n",
        "#can change to 'mnist'\n",
        "\n",
        "digit = None \n",
        "#can change to any number between 0 to 9 [for generating a specific class]\n",
        "\n",
        "batch_size = 128\n",
        "num_iter = 40000\n",
        "\n",
        "batch_size_min = 64 #nu\n",
        "decay_rate = 0.99 #gamma\n",
        "\n",
        "lr = 0.0002\n",
        "momentum = 0.5\n",
        "momentum2 = 0.99\n",
        "\n",
        "loss_function = BCE_from_logits\n",
        "\n",
        "top_k = 1 #if 0 top-k method will not be used\n",
        "\n",
        "dataset_loaders = {'mnist': mnist_dataset, 'cifar10': cifar10_dataset}\n",
        "generators = {'mnist': conv_generator_mnist, 'cifar10': conv_generator_cifar10}\n",
        "\n",
        "g_input_shape=(100,)\n",
        "d_input_shapes={'mnist': (28,28,1), 'cifar10': (32,32,3)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpc_GHOW66tz"
      },
      "source": [
        "#Helper methods for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi0kz2J_sHCh"
      },
      "source": [
        "helper method to initialize gan. prng is a jax.random.PRNGKey used for pseudo-random layer initialization of discriminator and generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meIaF34VsIxU"
      },
      "source": [
        "def create_and_initialize_gan(prng):\n",
        "    d_creator = conv_discriminator\n",
        "    g_creator = generators[dataset]\n",
        "    d_opt_creator = partial(adam, lr, momentum, momentum2)\n",
        "    g_opt_creator = partial(adam, lr, momentum, momentum2)\n",
        "\n",
        "    gan = GAN(d_creator, g_creator, d_opt_creator, g_opt_creator, \n",
        "              loss_function)\n",
        "\n",
        "    prng1, prng2 = jax.random.split(prng, 2)\n",
        "    d_state, g_state = gan.init(prng1, prng2, d_input_shapes[dataset], \n",
        "                                g_input_shape, batch_size)\n",
        "    return gan, d_state, g_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "munTziURr_cM"
      },
      "source": [
        "helper method to show samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5095dfIQr_Db"
      },
      "source": [
        "def plot_samples(ims, grid_dim):\n",
        "    dim1 = grid_dim\n",
        "    dim2 = len(ims)//dim1\n",
        "\n",
        "    for i in range(len(ims)):\n",
        "        im = ims[i]\n",
        "        if im.shape[2] == 1:\n",
        "            im = im.reshape(im.shape[:2])\n",
        "        plt.subplot(dim1, dim2, i + 1)\n",
        "        plt.imshow((im + 1.0) / 2.0)\n",
        "        plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68M6JxMy14eZ"
      },
      "source": [
        "#The training method. \n",
        "\n",
        "Uncommnent \"clear_output\" in order not to create a long output of images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAyXDaHpslSD"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def train():\n",
        "\n",
        "    dataset_loader = dataset_loaders[dataset]\n",
        "    real_data = dataset_loader(batch_size, digit=digit)\n",
        "    \n",
        "\n",
        "    prng = jax.random.PRNGKey(0)\n",
        "    prng_to_use, prng = jax.random.split(prng, 2)\n",
        "    gan, d_state, g_state = create_and_initialize_gan(prng_to_use)\n",
        "\n",
        "    d_losses = []\n",
        "    g_losses = []\n",
        "\n",
        "    start_time = time.time()\n",
        "    i = 0\n",
        "\n",
        "    prng_images, prng = jax.random.split(prng, 2)\n",
        "    z = jax.random.normal(prng_images, (9, 100))\n",
        "\n",
        "    k = batch_size\n",
        "    while i < num_iter:\n",
        "        epoch_start_time = time.time()\n",
        "        for real_ims, _ in real_data:\n",
        "            if i >= num_iter:\n",
        "                break\n",
        "\n",
        "            prng, prng_to_use = jax.random.split(prng, 2)\n",
        "            d_state, g_state, d_loss_value, g_loss_value = gan.train_step(i, \n",
        "                                   prng_to_use, d_state, g_state, real_ims, k)\n",
        "            d_losses.append(d_loss_value)\n",
        "            g_losses.append(g_loss_value)\n",
        "            i = i + 1\n",
        "        # clear_output()\n",
        "        print(f'iter{i}/{num_iter}')\n",
        "        print(f'epoch finished in {time.time() - epoch_start_time}second')\n",
        "        plot_samples(gan.generate_samples(z, g_state), 3)\n",
        "        if top_k == 1:\n",
        "            k = int(k * decay_rate)\n",
        "            k = max(batch_size_min, k)\n",
        "            print(f\"iter:{i}/{num_iter}, updated k: {k}\")\n",
        "    print(f'finished, took{time.time() - start_time}')\n",
        "\n",
        "    return d_losses, g_losses, d_state, g_state, gan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heUxW-_xr82Y"
      },
      "source": [
        "# request to use TPU \n",
        "Uncomment to use TPU. You must change runtime type to TPU from Runtime option. Or do not execute this part to run on CPU or GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0hbF2AvtAsf"
      },
      "source": [
        "# import os\n",
        "# import requests\n",
        "\n",
        "# if 'TPU_DRIVER_MODE' not in globals():\n",
        "#   url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver_nightly'\n",
        "#   resp = requests.post(url)\n",
        "#   TPU_DRIVER_MODE = 1\n",
        "\n",
        "# from jax.config import config\n",
        "# config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "# config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
        "# print(config.FLAGS.jax_backend_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwFguP7qtaBD"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pw6C6XwtZiA"
      },
      "source": [
        "d_losses, g_losses, d_state, g_state, gan = train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lJFUFFjwzXE"
      },
      "source": [
        "# Plot loss history"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZCPrZYHw2Oj"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(d_losses, label=\"d_loss\", alpha=0.5)\n",
        "plt.plot(g_losses, label=\"d_loss\", alpha=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdYgbcFFxAP8"
      },
      "source": [
        "# Save Gan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZAez-2nw6nu"
      },
      "source": [
        "path_to_GAN = \"./tmp/gan.pkl\"\n",
        "GAN.save_gan_to_file(gan, d_state, g_state, path_to_GAN)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feIkHvP1avir"
      },
      "source": [
        "#Load Gan and Generate Images\n",
        "\n",
        "Any trained model can be used.\n",
        "Change number of iterations or noise dimention to generate fewer images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PgG_oWZamHM"
      },
      "source": [
        "import jax\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "seed = 0\n",
        "\n",
        "path_to_GAN = \"./tmp/gan.pkl\"\n",
        "path_to_images = \"./tmp/GAN_images/\"\n",
        "try:\n",
        "  os.makedirs(path_to_images)\n",
        "except OSError:\n",
        "  print(\"WARNING: path already existed\")\n",
        "\n",
        "\n",
        "gan, d_state, g_state = GAN.load_gan_from_file(path_to_GAN)\n",
        "\n",
        "prng = jax.random.PRNGKey(seed)\n",
        "\n",
        "count = 0\n",
        "for i in range(50):\n",
        "  print(i)\n",
        "  prng_to_use, prng = jax.random.split(prng)\n",
        "  z = jax.random.normal(prng, (1000,100))\n",
        "\n",
        "  ims = gan.generate_samples(z, g_state)\n",
        "  ims.block_until_ready()\n",
        "  ims = np.array(ims)\n",
        "  plt.imshow((ims[347].reshape(32,32,3)+1.0)/2.0)\n",
        "  plt.show()\n",
        "  for j in range(1000):\n",
        "    #clipping because the results sometimes are rounded to epsilon higher than 1\n",
        "    im = np.clip((ims[j].reshape(32,32,3)+1.0)/2.0, 0, 1)\n",
        "    plt.imsave(path_to_images+'{:05d}.png'.format(count),im)\n",
        "    count += 1\n",
        "    print(f\"generated {count}/{50000}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxwS0RIWb9ME"
      },
      "source": [
        "#FID\n",
        "download and add the CIFAR FID statistics folder to \"./tmp/\".\n",
        "\n",
        "FID might not work due to limited memroy.\n",
        "Reducing --batch-size or\n",
        "connecting to Google Drive and saving the output images on google drive might help."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOTqPIsabvfA"
      },
      "source": [
        "! pip install pytorch-fid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYsGa7Pdaljq"
      },
      "source": [
        "! python -m pytorch_fid  \"./tmp/GAN_images\" \"./tmp/fid_stats_cifar10_train.npz\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kUR9PfCtESR"
      },
      "source": [
        "# END"
      ]
    }
  ]
}